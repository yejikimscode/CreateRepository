{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK 패키지는 아나콘다에 기본으로 포함되어 있으나 필요한 세부 패키지는 따로 다운로드 해야함\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('webtext')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 토큰화(sentence tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize # sent_tokenize는 영어학습 데이터에 대해 사전학습된 모델을 사용해 토큰화\n",
    "print(sent_tokenize(para)) # 주어진 text를 sentence 단위로 tokenize함. 주로 . ! ? 을 이용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non.\", \"Je t'ai demandé si j'étais jolie, Tu m'a répondu non.\", \"Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"]\n"
     ]
    }
   ],
   "source": [
    "paragraph_french = \"\"\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non. \n",
    "Je t'ai demandé si j'étais jolie, Tu m'a répondu non. \n",
    "Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"\"\"\n",
    "\n",
    "# import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')  # 프랑스어 사전학습 모델 로드\n",
    "print(tokenizer.tokenize(paragraph_french))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다!']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(para_kor)) \n",
    "#NLTK에 한국어 사전학습 모델은 아직 없지만 문장분리 방법은 비슷하여 한국어에 대해서도 sentence tokenizer는 잘 동작함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 토큰화 (word tokenize)\n",
    "단어 토큰화를 위해 문장 토큰화를 수행할 필요는 없음. 서로 다른 알고리즘의 토크나이저 특성을 이해하고 분석 목적에 맞는 토크나이저 선택할 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(para)) #주어진 text를 공백과 구두점을 기준으로 분리.  # It's  => 'It', \"'s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "print(WordPunctTokenizer().tokenize(para))  # It's => 'It', \"'\", 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '텍스트마이닝', '클래스를', '시작해봅시다', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(para_kor))    # 영어는 주로 공백과 구두점을 기준으로 단어 토큰화. 교착어인 한국어는?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규표현식을 이용한 토큰화\n",
    "NLTK 제공 함수를 이용한 토큰화는 간편하지만 정교한 토큰화는 어려움  \n",
    "정규표현식을 이용하면 NLTK 없이도 다양한 조건으로 토큰화 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  # regular expression (정규표현식)   # 위키독스 점프 투 파이썬 7장\n",
    "\n",
    "# 정규표현식은 메타 문자로 패턴 표현. 가장 기본적인 메타문자: 문자 클래스 [ ] \n",
    "re.findall(\"[abc]\", \"How are you, boy?\")   # [abc] a or b or c 중 일치하는 문자 추출. 첫째 인수의 패턴을 둘째 인수인 문자열에서 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7', '5', '9']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[0123456789]\", \"3a7b5c9d\")   # [0123456789] 대신 [0-9]로 해도 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', 'a', '7', 'b', '_', '5', 'c', '9', 'd']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[\\w]\", \"3a 7b_ '.^&5c9d\")   # [\\w] 은 [a-zA-Z0-9_]와 동일(소/대문자 알파벳, 숫자, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_', '__', '___']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규표현식 메타 문자 +  한 번 이상의 반복을 의미\n",
    "re.findall(\"[_]+\", \"a_b, c__d, e___f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'are', 'you', 'boy']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[\\w]+\", \"How are you, boy?\")   # [\\w] 에는 공백이나 쉼표 등이 포함되지 않음. 이 특징을 이용하면 단어 추출 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oo', 'oooo', 'oooo', 'ooo']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규표현식 메타 문자 {} 정확한 반복 횟수 지정\n",
    "re.findall(\"[o]{2,4}\", \"oh, hoow are yoooou, boooooooy?\")   # o 가 2~4번 반복 패턴 찾기. boooooooy 는 최장 매칭 oooo 와 ooo 로 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK에서는 정규표현식을 사용하는 토크나이저를 RegexpTokenizer 로 제공**  \n",
    "RegexpTokenizer() 함수 인자로 원하는 정규표현식을 주면 그에 따라 토큰화 수행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sorry', 'I', \"can't\", 'go', 'there']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# \\w (대소문자 알파벳, 숫자, _) 와 아포스트로피 ' 를 기준으로 이것들이 한 번 이상 반복되는 패턴 찾아 토큰화 \n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")   \n",
    "\n",
    "print(tokenizer.tokenize(\"Sorry, I can't go there.\"))   # can't를 하나의 단어로 인식(' 도 단어 구성 요소로 간주하는 경우)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sorry', 'I', 'can', 't', 'go', 'there']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w]+\")   # 위 예에서 ' 제외\n",
    "print(tokenizer.tokenize(\"Sorry, I can't go there.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', \"can't\", 'there']\n"
     ]
    }
   ],
   "source": [
    "# 텍스트를 먼저 모두 소문자로 바꾸고 '를 포함해 세 글자 이상의 단어들만 추출하도록 \n",
    "text1 = \"Sorry, I can't go there.\"\n",
    "tokenizer = RegexpTokenizer(\"[\\w']{3,}\") \n",
    "print(tokenizer.tokenize(text1.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정제(Cleaning)\n",
    "### 노이즈와 불용어 제거\n",
    "위 예에서 정규표현식을 이용한 토큰화를 통해 특수문자와 같은 불필요한 문자들 혹은 노이즈를 삭제할 수 있었음.  \n",
    "토큰화 과정과 별도로 **정규표현식을 이용한 치환을 통해 원하는 패턴의 노이즈를 제거**할 수도 있음.  \n",
    "\n",
    "NLTK에서는 stopwords 라는 라이브러리를 이용해 언어별 불용어 사전 제공  \n",
    "stopwords(불용어): 의미없는 특수문자 등과는 별도로 실제 사용되는 단어지만 분석에는 별 필요가 없는 단어들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.ewha.ac.kr/ewha/index.do\n",
      "['www.ewha.ac.kr', 'ewha', 'index.do']\n",
      "www.ewha.ac.kr\n"
     ]
    }
   ],
   "source": [
    "a = \"http://www.ewha.ac.kr/ewha/index.do\"\n",
    "a = a.replace(\"http://\", \"\")\n",
    "print(a)\n",
    "\n",
    "print(a.split(\"/\"))\n",
    "print((a.split(\"/\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'l', 'p', 'h', 'a', 'n', 'u', 'm', 'e', 'r', 'i', 'c', '1', '2', '3']\n",
      "alphanumeric123\n"
     ]
    }
   ],
   "source": [
    "string_value = \"alphanumeric@123__\"\n",
    "\n",
    "print([ch for ch in string_value if ch.isalnum()])\n",
    "\n",
    "s = ''.join(ch for ch in string_value if ch.isalnum())\n",
    "\n",
    "print(s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphanumeric123\n"
     ]
    }
   ],
   "source": [
    "string_value = \"alphanumeric@123__\"\n",
    "s = ''.join(filter(str.isalnum, string_value))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphanumeric123\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string_value = \"alphanumeric@123__\"\n",
    "s=re.sub(r'[\\W_]+', '', string_value)\n",
    "# s=re.sub(r'[\\W]+', '', string_value)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphanumeric123\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string_value = \"alphanumeric@123__\"\n",
    "s = re.sub(r'[^a-zA-Z0-9]', '', string_value)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "park 010-1234-4567\n",
      "park\n",
      "1234\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식 메타 문자 ()는 그룹을 의미  group(0): 매치된 전체 문자열, group(1): 첫 번째 그룹의 문자열 \n",
    "\n",
    "import re\n",
    "\n",
    "p = re.compile(r\"(\\w+)\\s+\\d+[-](\\d+)[-]\\d+\")   # r raw string 의미 \\ 이스케이프 문자 문제 해결, \\s 스페이스, \\d 숫자\n",
    "m = p.search(\"park 010-1234-4567\")\n",
    "print(m.group(0))\n",
    "print(m.group(1))\n",
    "print(m.group(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  Hello ,  everyone^^ How are you ?  It's good to see you~~ Let's start our text mining class** ! \n",
      "text:  Hello , everyone How are you ? It s good to see you Let s start our text mining class ! \n",
      "\n",
      "tokens:  ['Hello', ',', 'everyone', 'How', 'are', 'you', '?', 'It', 's', 'good', 'to', 'see', 'you', 'Let', 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식을 이용한 노이즈 제거 예시 \n",
    "\n",
    "import re\n",
    "\n",
    "text = \"Hello, everyone^^ How are you? It's good to see you~~ Let's start our text mining class**!\"\n",
    "\n",
    "text = re.sub(r\"([.,!?])\", r\" \\1 \", text)   # 정규표현식에서 () 는 그룹을 의미, \\1 은 첫번째 그룹을 의미, 구두점 앞뒤에 공백 넣기\n",
    "print('text: ', text)\n",
    "\n",
    "text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)   # 정규표현식에서 [] 안의 ^는 아님을 의미  \n",
    "print('text: ', text)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"tokens: \", text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello , everyone how are you ? it s good to see you let s start our text mining class ! \n"
     ]
    }
   ],
   "source": [
    "# 위 노이즈 제거 과정을 텍스트 전처리 함수로 만든 예시\n",
    "\n",
    "text = \"Hello, everyone^^ How are you? It's good to see you~~ Let's start our text mining class**!\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "\n",
    "print(preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello , everyone how are you ? it's good to see you let's start our text mining class ! \n"
     ]
    }
   ],
   "source": [
    "# 위 텍스트 전처리 함수를 어포스트로피 ' 를 삭제하지 않도록 수정\n",
    "\n",
    "text = \"Hello, everyone^^ How are you? It's good to see you~~ Let's start our text mining class**!\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?']+\", r\" \", text)   # 어포스트로피 ' 를 삭제하지 않도록 [] 안에 '  추가\n",
    "    return text\n",
    "\n",
    "print(preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'go', 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "#  stopwords 라이브러리 사용 불용어 제거\n",
    "from nltk.corpus import stopwords \n",
    "english_stops = set(stopwords.words('english')) # 반복이 되지 않도록 set으로 변환\n",
    "\n",
    "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(text1.lower()) # word_tokenize로 토큰화\n",
    "\n",
    "# 여기서는 영어 텍스트 토큰화를 하고 불용어를 제거함\n",
    "result = [word for word in tokens if word not in english_stops] # stopwords를 제외한 단어들만으로 list를 생성\n",
    "print(result)   # I, couldn't, to 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'up', 'once', 'mightn', 'yourself', 'has', 'whom', 'that', 'after', 'the', 'same', \"couldn't\", \"doesn't\", \"mustn't\", 'been', 'some', 'is', 'being', 'hadn', 'too', 'am', 'she', 'out', 'under', \"didn't\", 'wasn', 'about', 'above', 'their', \"you're\", 'herself', 't', 'did', 'ours', 'wouldn', 'was', 'having', 'they', 'we', 'ain', 'o', 'by', \"haven't\", 'her', 'an', 's', 'him', 'have', 'should', 'it', 'nor', 'don', \"aren't\", 'doing', 'doesn', 'needn', 'few', \"don't\", 'them', 'when', 'in', 'isn', 'yours', 'shan', 'myself', 'again', \"it's\", 'before', 'weren', 'other', 'down', \"you've\", \"weren't\", \"won't\", 'will', 'more', 'were', \"you'd\", 'what', 'his', 'over', 'a', 'your', \"shan't\", 'until', 'where', 'each', 'than', 'shouldn', 'there', 'all', 'to', 'me', 'with', 'between', 'such', \"isn't\", 'then', 'so', 'because', \"should've\", 'do', 'd', 'from', 'who', 'into', 've', 'not', 'didn', 'against', 'hasn', 'this', 'hers', 'you', 'at', 'most', 'm', 'ourselves', 'won', \"you'll\", 'off', 'can', 'mustn', 'ma', 'itself', 'both', \"that'll\", 're', 'on', 'which', 'very', \"wouldn't\", 'i', 'themselves', 'just', \"wasn't\", 'of', 'why', 'now', 'my', 'does', 'yourselves', \"mightn't\", 'are', 'any', \"hadn't\", 'through', 'if', \"shouldn't\", 'own', 'these', 'y', 'theirs', 'had', 'himself', 'only', 'he', 'or', 'and', 'how', \"hasn't\", 'those', 'its', 'but', 'haven', 'here', 'while', 'during', 'for', 'couldn', \"needn't\", 'no', 'be', \"she's\", 'below', 'further', 'aren', 'our', 'll', 'as'}\n"
     ]
    }
   ],
   "source": [
    "print(english_stops) # nltk가 제공하는 영어 stopword를 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자신만의 불용어 사전 만들고 활용하기 - *한글처리에서도 유용하게 사용할 수 있음*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', \"couldn't\", 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "my_stopword = ['i', 'go', 'to'] # 나만의 stopword를 리스트로 정의\n",
    "result = [word for word in tokens if word not in my_stopword] \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규화(Normalization)\n",
    "### 어간 추출(Stemming)\n",
    "스테머 알고리즘은 단어가 변형되는 규칙을 이용해 어간을 추출하며 그 결과가 항상 사전에 있는 단어가 되지는 않음  \n",
    "그렇지만 특정 스테머 함수를 적용하면 모든 단어가 같은 규칙에 따라 변환됨  \n",
    "변환된 단어가 사전에 없는 단어라도 동일한 규칙에 의해 변환되었으므로 분석 의도를 충족시킬 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookeri cookbook\n",
      "connect connect connect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))  \n",
    "# cookery => cookeri : y를 i로 대체하는 규칙에 따라 생성된 것으로 사전에 있는 단어 아님\n",
    "\n",
    "print(stemmer.stem('connected'), stemmer.stem('connecting'), stemmer.stem('connection'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화 후 어간 추출 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
      "['hello', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'mine', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "tokens = word_tokenize(para) # 토큰화 실행\n",
    "print(tokens)\n",
    "result = [stemmer.stem(token) for token in tokens] # 모든 토큰에 대해 스테밍 실행\n",
    "print(result)\n",
    "# everyone => everyon, mining => mine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "포터 스테머(PorterStemmer)와 랭카스터 스테머(LancasterStemmer) 결과가 약간 다름  \n",
    "어떤 스테머를 선택할지는 스테밍 결과 및 최종 결과를 비교해 보고 분석 목적에 더욱 적합한 스테머 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookery cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))\n",
    "# cookery => cookery "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본형(표제어) 추출(Lemmatization)\n",
    "사전을 이용해 사전에 있는 기본형 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cookery\n",
      "cookbook\n",
      "cook\n",
      "cookery\n",
      "cookbooks\n",
      "am\n",
      "ha\n",
      "watched\n",
      "doing\n",
      "be\n",
      "have\n",
      "watch\n",
      "do\n",
      "connected\n",
      "connecting\n",
      "connection\n",
      "connect\n",
      "connect\n",
      "connection\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cooking'))   # 사전에 cooking 이라는 명사가 존재하기 때문에 품사를 지정하지 않으면 cooking 봔환 \n",
    "print(lemmatizer.lemmatize('cookery'))\n",
    "print(lemmatizer.lemmatize('cookbooks'))\n",
    "\n",
    "print(lemmatizer.lemmatize('cooking', pos='v')) # pos 인자에 동사를 의미하는 'v' 로 품사를 지정하면 cook 반환 \n",
    "print(lemmatizer.lemmatize('cookery', pos='v'))\n",
    "print(lemmatizer.lemmatize('cookbooks', pos='v'))\n",
    "\n",
    "print(lemmatizer.lemmatize('am'))\n",
    "print(lemmatizer.lemmatize('has'))\n",
    "print(lemmatizer.lemmatize('watched'))\n",
    "print(lemmatizer.lemmatize('doing'))\n",
    "\n",
    "print(lemmatizer.lemmatize('am', pos='v'))\n",
    "print(lemmatizer.lemmatize('has', pos='v'))\n",
    "print(lemmatizer.lemmatize('watched', pos='v'))\n",
    "print(lemmatizer.lemmatize('doing', pos='v'))\n",
    "\n",
    "print(lemmatizer.lemmatize('connected'))\n",
    "print(lemmatizer.lemmatize('connecting'))\n",
    "print(lemmatizer.lemmatize('connection'))\n",
    "\n",
    "print(lemmatizer.lemmatize('connected', pos='v'))\n",
    "print(lemmatizer.lemmatize('connecting', pos='v'))\n",
    "print(lemmatizer.lemmatize('connection', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming result: believ\n",
      "lemmatizing result: belief\n",
      "lemmatizing result: believe\n"
     ]
    }
   ],
   "source": [
    "#comparison of lemmatizing and stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print('stemming result:', stemmer.stem('believes'))\n",
    "print('lemmatizing result:', lemmatizer.lemmatize('believes'))\n",
    "print('lemmatizing result:', lemmatizer.lemmatize('believes', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 전처리 내용 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 띄어쓰기 및 맞춤법 교정(Py-Hanspell)\n",
    "Py-Hanspell은 네이버 한글 맞춤법 검사기를 바탕으로 만들어진 패키지. 띄어쓰기도 교정함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anaconda Prompt 를 관리자 권한으로 실행하여 터미널 창에 아래 명령어 입력하여 실행\n",
    "# conda install git pip \n",
    "# pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지\n"
     ]
    }
   ],
   "source": [
    "from hanspell import spell_checker\n",
    "\n",
    "sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\n",
    "spelled_sent = spell_checker.check(sent)\n",
    "\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 토큰화 및 형태소 분석 (영어 NLTK 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''절망의 반대가 희망은 아니다.\n",
    "어두운 밤하늘에 별이 빛나듯\n",
    "희망은 절망 속에 싹트는 거지\n",
    "만약에 우리가 희망함이 적다면\n",
    "그 누가 세상을 비출어줄까.\n",
    "정희성, 희망 공부'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['절망의', '반대가', '희망은', '아니다', '.', '어두운', '밤하늘에', '별이', '빛나듯', '희망은', '절망', '속에', '싹트는', '거지', '만약에', '우리가', '희망함이', '적다면', '그', '누가', '세상을', '비출어줄까', '.', '정희성', ',', '희망', '공부']\n",
      "\n",
      "[('절망의', 'JJ'), ('반대가', 'NNP'), ('희망은', 'NNP'), ('아니다', 'NNP'), ('.', '.'), ('어두운', 'VB'), ('밤하늘에', 'JJ'), ('별이', 'NNP'), ('빛나듯', 'NNP'), ('희망은', 'NNP'), ('절망', 'NNP'), ('속에', 'NNP'), ('싹트는', 'NNP'), ('거지', 'NNP'), ('만약에', 'NNP'), ('우리가', 'NNP'), ('희망함이', 'NNP'), ('적다면', 'NNP'), ('그', 'NNP'), ('누가', 'NNP'), ('세상을', 'NNP'), ('비출어줄까', 'NNP'), ('.', '.'), ('정희성', 'NN'), (',', ','), ('희망', 'NNP'), ('공부', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print()\n",
    "print(nltk.pos_tag(tokens))   # nltk.pos_tag()은 토큰화된 결과에 대해 품사를 태깅해 (단어, 품사) 튜플의 리스트 반환 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 형태소 분석 (KoNLPy 사용)\n",
    "\n",
    "KoNLPy 설치는 다음 강의에서 안내"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소: ['절망', '의', '반대', '가', '희망', '은', '아니다', '.', '\\n', '어', '두운', '밤하늘', '에', '별', '이', '빛나듯', '\\n', '희망', '은', '절망', '속', '에', '싹트는', '거지', '\\n', '만약', '에', '우리', '가', '희망', '함', '이', '적다면', '\\n', '그', '누가', '세상', '을', '비출어줄까', '.', '\\n', '정희성', ',', '희망', '공부']\n",
      "\n",
      "명사: ['절망', '반대', '희망', '어', '두운', '밤하늘', '별', '희망', '절망', '속', '거지', '만약', '우리', '희망', '함', '그', '누가', '세상', '정희성', '희망', '공부']\n",
      "\n",
      "품사 태깅 결과: [('절망', 'Noun'), ('의', 'Josa'), ('반대', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('은', 'Josa'), ('아니다', 'Adjective'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('어', 'Noun'), ('두운', 'Noun'), ('밤하늘', 'Noun'), ('에', 'Josa'), ('별', 'Noun'), ('이', 'Josa'), ('빛나듯', 'Verb'), ('\\n', 'Foreign'), ('희망', 'Noun'), ('은', 'Josa'), ('절망', 'Noun'), ('속', 'Noun'), ('에', 'Josa'), ('싹트는', 'Verb'), ('거지', 'Noun'), ('\\n', 'Foreign'), ('만약', 'Noun'), ('에', 'Josa'), ('우리', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('함', 'Noun'), ('이', 'Josa'), ('적다면', 'Verb'), ('\\n', 'Foreign'), ('그', 'Noun'), ('누가', 'Noun'), ('세상', 'Noun'), ('을', 'Josa'), ('비출어줄까', 'Verb'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('정희성', 'Noun'), (',', 'Punctuation'), ('희망', 'Noun'), ('공부', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "print('형태소:', t.morphs(sentence))\n",
    "print()\n",
    "print('명사:', t.nouns(sentence))\n",
    "print()\n",
    "print('품사 태깅 결과:', t.pos(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
