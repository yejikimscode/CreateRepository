{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK를 이용한 영어 텍스트 품사 태깅\n",
    "NLTK는 먼저 토큰화를 하고 토큰들에 대한 품사 태깅을 수행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), ('!', '.')]\n",
      "[('It', 'PRP'), (\"'s\", 'VBZ'), ('an', 'DT'), ('unexpected', 'JJ'), ('thing', 'NN'), ('.', '.')]\n",
      "[('unexpected', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(\"Hello everyone. It's good to see you. Let's start our text mining class!\")\n",
    "\n",
    "print(nltk.pos_tag(tokens))   # 품사 태깅 결과를 (단어, 품사)로 구성된 튜플의 리스트로 반환\n",
    "\n",
    "\n",
    "tokens = word_tokenize(\"It's an unexpected thing.\")\n",
    "print(nltk.pos_tag(tokens))                       \n",
    "print(nltk.pos_tag(['unexpected']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK는 펜 트리뱅크 태그 세트를 사용하므로 해당 표를 보면 품사 약어의 의미를 알 수 있음  \n",
    "아래와 같이 품사 약어의 의미와 설명을 볼 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('CC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 품사만을 추출하여 분석할 경우는 다음 예시 코드 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), ('!', '.')]\n",
      "my_words:  ['everyone', 'good', 'see', 'Let', 'start', 'text', 'mining', 'class']\n"
     ]
    }
   ],
   "source": [
    "my_tag_set = ['NN', 'VB', 'JJ']\n",
    "\n",
    "tokens = word_tokenize(\"Hello everyone. It's good to see you. Let's start our text mining class!\")\n",
    "\n",
    "print(nltk.pos_tag(tokens))\n",
    "\n",
    "my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]\n",
    "\n",
    "print('my_words: ', my_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동음이의어를 처리하거나 품사를 이용해 단어를 더욱 정확하게 구분하고 싶을 때는 다음과 같이 단어 뒤에 품사 태그를 붙여 사용  \n",
    "BOW(Bag Of Words)를 이용한 문서 분류에서 품사 정보 추가시 성능 차이가 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello/NNP', 'everyone/NN', './.', 'It/PRP', \"'s/VBZ\", 'good/JJ', 'to/TO', 'see/VB', 'you/PRP', './.', 'Let/VB', \"'s/POS\", 'start/VB', 'our/PRP$', 'text/NN', 'mining/NN', 'class/NN', '!/.']\n"
     ]
    }
   ],
   "source": [
    "words_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokens)]   # 단어와 토큰을 붙여 서로 다른 품사의 같은 형태 단어를 다른 단어로 \n",
    "print(words_with_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoNLPy를 이용한 한국어 텍스트 형태소 분석 및 품사 태깅\n",
    "KoNLPy는 토큰화를 미리 하지 않고 형태소 분석 및 품사 태깅 함수가 토큰화를 함께 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소: ['절망', '의', '반대', '가', '희망', '은', '아니다', '.', '\\n', '어', '두운', '밤하늘', '에', '별', '이', '빛나듯', '\\n', '희망', '은', '절망', '속', '에', '싹트는', '거지', '\\n', '만약', '에', '우리', '가', '희망', '함', '이', '적다면', '\\n', '그', '누가', '세상', '을', '비출어줄까', '.', '\\n', '정희성', ',', '희망', '공부']\n",
      "\n",
      "명사: ['절망', '반대', '희망', '어', '두운', '밤하늘', '별', '희망', '절망', '속', '거지', '만약', '우리', '희망', '함', '그', '누가', '세상', '정희성', '희망', '공부']\n",
      "\n",
      "품사 태깅 결과: [('절망', 'Noun'), ('의', 'Josa'), ('반대', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('은', 'Josa'), ('아니다', 'Adjective'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('어', 'Noun'), ('두운', 'Noun'), ('밤하늘', 'Noun'), ('에', 'Josa'), ('별', 'Noun'), ('이', 'Josa'), ('빛나듯', 'Verb'), ('\\n', 'Foreign'), ('희망', 'Noun'), ('은', 'Josa'), ('절망', 'Noun'), ('속', 'Noun'), ('에', 'Josa'), ('싹트는', 'Verb'), ('거지', 'Noun'), ('\\n', 'Foreign'), ('만약', 'Noun'), ('에', 'Josa'), ('우리', 'Noun'), ('가', 'Josa'), ('희망', 'Noun'), ('함', 'Noun'), ('이', 'Josa'), ('적다면', 'Verb'), ('\\n', 'Foreign'), ('그', 'Noun'), ('누가', 'Noun'), ('세상', 'Noun'), ('을', 'Josa'), ('비출어줄까', 'Verb'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('정희성', 'Noun'), (',', 'Punctuation'), ('희망', 'Noun'), ('공부', 'Noun')]\n",
      "[('강아지', 'Noun'), ('가', 'Josa'), ('아파서', 'Adjective'), ('약', 'Noun'), ('을', 'Josa'), ('먹이', 'Noun'), ('다', 'Josa'), ('.', 'Punctuation')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('학생', 'Noun'),\n",
       " ('들', 'Suffix'),\n",
       " ('부터', 'Josa'),\n",
       " ('많이', 'Adverb'),\n",
       " ('잡혀', 'Verb'),\n",
       " ('들어갔다', 'Verb')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '''절망의 반대가 희망은 아니다.\n",
    "어두운 밤하늘에 별이 빛나듯\n",
    "희망은 절망 속에 싹트는 거지\n",
    "만약에 우리가 희망함이 적다면\n",
    "그 누가 세상을 비출어줄까.\n",
    "정희성, 희망 공부'''\n",
    "\n",
    "print('형태소:', t.morphs(sentence))\n",
    "print()\n",
    "print('명사:', t.nouns(sentence))\n",
    "print()\n",
    "print('품사 태깅 결과:', t.pos(sentence))\n",
    "\n",
    "sentence = \"강아지가 아파서 약을 먹이다.\"\n",
    "print(t.pos(sentence))\n",
    "\n",
    "sentence = \"학생들부터 많이 잡혀 들어갔다\"\n",
    "t.pos(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 텍스트 문장 단위 형태소분석\n",
    "\n",
    ">영어는 띄어쓰기를 기준으로 토큰화해도 단어의 최소 의미 단위와 비교적 일치하나 **한국어는 교착어 특징**을 갖기 때문에 띄어쓰기가 아닌 형태소를 기준으로 토큰화할 때가 많다.\n",
    "\n",
    ">따라서 한국어 텍스트에 대한 **최소 의미 단위인 형태소 기준 토큰화** 관련 작업들에 대해 좀 더 살펴본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'눈이\", \"부시게'가\", '가뿐하게', '지상파', '월화극을', '따돌리며', '6%를', '돌파했다.', '27일', '시청률', '조사회사', '닐슨', '코리아에', '따르면', '26일', '방송된', 'JTBC', '월화극', \"'눈이\", \"부시게'는\", '6.567%(전국', '유료가구', '기준)의', '시청률을', '기록했다.', '5회', '연속', '자체', '최고', '시청률을', '찍으며', '멈출', '줄', '모르는', '상승세를', '이어가고', '있다.', '동시에', '첫', '6%대', '돌파였다.', '동', '시간대', '방송된', '지상파', '3사', '월화극', 'SBS', \"'해치'\", 'KBS', '2TV', \"'동네변호사\", '조들호2:죄와', \"벌'\", 'MBC', \"'아이템'을\", '따돌리고', '우위를', '점했다.', 'tvN', \"'왕이\", '된', \"남자'(9.5%)를\", '잇는', '월화극', '전체', '2위에', '이름을', '올렸다.', \"'왕이\", '된', \"남자'의\", '경우', '종영을', '앞두고', '있기에', \"'눈이\", \"부시게'가\", '어디까지', '상승할', '수', '있을지', '주목된다.', '이날', '방송에는', '김혜자(김혜자)가', '방송', '말미', '시간을', '되돌리는', '시계를', '발견하는', '모습이', '그려졌다.', '전무송이', '이', '시계를', '차고', '있었고', '시계를', '본', '후', '눈빛이', '심하게', '흔들린', '김혜자의', '모습을', '통해', '다시금', '시간을', '되돌릴', '수', '있을지', '여부에', '관심이', '쏠렸다.']\n"
     ]
    }
   ],
   "source": [
    "# input_file_name = \"./Data/textdatatextdata_utf8.txt\"\n",
    "input_file_name = r\".\\Data\\textdata_utf8.txt\"\n",
    "\n",
    "# with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "# with open(input_file_name, \"r\", encoding=\"ANSI\") as input_file:\n",
    "with open(input_file_name, \"r\", encoding=\"utf8\") as input_file:\n",
    "\n",
    "    text_words = []\n",
    "\n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        words = line.split() # 일반적으로 단어 구분은 띄어쓰기(공백) 기준으로 이뤄지기 때문에 띄어쓰기 기준으로 분절함.\n",
    "        text_words += words\n",
    "\n",
    "print(text_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['봄이', '왔어요']\n",
      "['봄이', '왔어요', '진달래', '개나리']\n"
     ]
    }
   ],
   "source": [
    "line = \"봄이 왔어요\"\n",
    "words = line.split()\n",
    "print(words)\n",
    "\n",
    "a=[\"진달래\", \"개나리\"]\n",
    "print(words + a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitlines() 메소드 사용\n",
    "\n",
    "`splitlines()` 메소드는 줄바꿈을 기준으로 문자열을 여러 개로 나눈뒤 리스트로 반환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죽는 날까지 하늘을 우러러\n",
      "한 점 부끄럼이 없기를,\n",
      "잎새에 이는 바람에도\n",
      "나는 괴로워했다.\n",
      "\n",
      "['죽는 날까지 하늘을 우러러', '한 점 부끄럼이 없기를,', '잎새에 이는 바람에도', '나는 괴로워했다.']\n",
      "\n",
      "죽는 날까지 하늘을 우러러\n"
     ]
    }
   ],
   "source": [
    "poem = \"\"\"죽는 날까지 하늘을 우러러\n",
    "한 점 부끄럼이 없기를,\n",
    "잎새에 이는 바람에도\n",
    "나는 괴로워했다.\n",
    "\"\"\"\n",
    "print(poem)\n",
    "\n",
    "lines = poem.splitlines()\n",
    "\n",
    "print(lines)\n",
    "print()\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죽는 날까지 하늘을 우러러\n",
      "한 점 부끄럼이 없기를,\n",
      "잎새에 이는 바람에도\n",
      "나는 괴로워했다.\n",
      "\n",
      "['죽는 날까지 하늘을 우러러', '한 점 부끄럼이 없기를,', '잎새에 이는 바람에도', '나는 괴로워했다.']\n"
     ]
    }
   ],
   "source": [
    "poem = \"\"\"죽는 날까지 하늘을 우러러\\n한 점 부끄럼이 없기를,\\n잎새에 이는 바람에도\\n나는 괴로워했다.\\n\"\"\"\n",
    "print(poem)\n",
    "\n",
    "lines = poem.splitlines()\n",
    "\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\n",
      "['죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.']\n"
     ]
    }
   ],
   "source": [
    "poem = \"\"\"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\"\"\"\n",
    "print(poem)\n",
    "\n",
    "lines = poem.splitlines()\n",
    "\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 토큰화\n",
    "\n",
    "사용 알고리즘마다 차이는 있으나, 일반적으로 단어 토큰화 전에 문서를 문장으로 분절하는 것은 영어나 한국어나 동일한 과정이다.\n",
    "\n",
    "문장을 구분하는 구분자들에는 **대표적으로, 마침표(.), 물음표(?), 느낌표(!)**가 있다.\n",
    "\n",
    "그런데 이들 구분자가 때로 문장의 종결이 아닌 곳에서도 사용될 수 있기 때문에 **이들 부호에 공백 문자가 연이어진 경우를 문장의 구분이 이루어지는 것으로 보는 것이 안전**하다. **실제 구현에 있어서는 문장의 구분을 줄의 구분과 일치시켜서 문장의 구분이 텍스트 파일의 외현적 구조에 반영되도록 하는 것이 편리**하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다.\", \"27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록했다.\", '5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다', '동시에 첫 6%대 돌파였다.']\n",
      "9\n",
      "5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다. 동시에 첫 6%대 돌파였다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\".\\Data\\textdata.txt\"\n",
    "         \n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    text_sentences = []\n",
    "\n",
    "    for line in input_file:\n",
    "        line = line.strip()  # 라인별로 읽음. \n",
    "        sentences = line.split(\". \")   # 마침표(.) 뒤에 스페이스가 있는 기준으로 분절함.\n",
    "        text_sentences +=sentences\n",
    "        \n",
    "print(text_sentences[:4])\n",
    "print(len(text_sentences))\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for i, line in enumerate(input_file):\n",
    "        if i== 2:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "input_file_name = r\".\\Data\\textdata.txt\"\n",
    "\n",
    "text_sentences=[]\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        sub_sentences = line.splitlines()\n",
    "        text_sentences +=sub_sentences\n",
    "print(len(text_sentences))\n",
    "\n",
    "text_sentences=[]\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "        line = line.replace(\". \", \".\\n\")\n",
    "        line = line.replace(\"? \", \"?\\n\")\n",
    "        line = line.replace(\"! \", \"!\\n\")   \n",
    "        sub_sentences = line.splitlines() \n",
    "        \n",
    "        text_sentences +=sub_sentences\n",
    "        \n",
    "print(len(text_sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 위 코드를 사용자 함수를 통해 재작성한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# 사용자 함수 작성\n",
    "\n",
    "def split_sentences(text):\n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "input_file_name = r\".\\Data\\textdata.txt\"\n",
    "\n",
    "text_sentences = []\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        sub_sentences = split_sentences(line)  # 앞서 정의한 사용자 함수 def split_sentences를 호출해 매개변수에 line을 입력\n",
    "        text_sentences += sub_sentences\n",
    "        \n",
    "print(len(text_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규표현식을 이용한 문장 토큰화\n",
    "\n",
    "아래 코드는 위 코드를 정규표현식과 사용자 정의 함수를 통해 재정리한 것이다.\n",
    "정규표현식을 사용하기 위해서는 `re` 모듈을 호출해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import re   # re 모듈 호출\n",
    "\n",
    "def split_sentences(text):\n",
    "#     sentences = re.split(\"(?<=[.?!])\\s+\", text.strip())\n",
    "    sentences = re.split(\"[.?!]\\s+\", text.strip())   # .나 ?나 ! 다음에 스페이스가 1개 이상 오는 패턴 \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "input_file_name = r\".\\Data\\textdata.txt\"\n",
    "\n",
    "text_sentences = []\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        sub_sentences = split_sentences(line)  # 앞의 사용자 정의 함수 def split_sentences를 호출해 인자에 line을 입력\n",
    "        text_sentences += sub_sentences\n",
    "        \n",
    "print(len(text_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('죽는', 'Verb'), ('날', 'Noun'), ('까지', 'Josa'), ('하늘', 'Noun'), ('을', 'Josa'), ('우러러', 'Noun'), ('한', 'Verb'), ('점', 'Noun'), ('부끄럼', 'Noun'), ('이', 'Josa'), ('없기를', 'Adjective'), (',', 'Punctuation'), ('잎새', 'Noun'), ('에', 'Josa'), ('이는', 'Verb'), ('바람', 'Noun'), ('에도', 'Josa'), ('나', 'Noun'), ('는', 'Josa'), ('괴로워', 'Adjective'), ('했다', 'Verb'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# from konlpy.tag import Twitter\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# twitter = Twitter()\n",
    "twitter = Okt()\n",
    "\n",
    "text = \"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\"\n",
    "\n",
    "text_pos = twitter.pos(text) # Twitter 모듈안에 있는 pos 사용자 함수를 호출  \n",
    "print(text_pos) # 리스트 내 튜플 형태(단어, 품사)로 출력 값 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죽는\tVerb\n",
      "날\tNoun\n",
      "까지\tJosa\n",
      "하늘\tNoun\n",
      "을\tJosa\n",
      "우러러\tNoun\n",
      "한\tVerb\n",
      "점\tNoun\n",
      "부끄럼\tNoun\n",
      "이\tJosa\n",
      "없기를\tAdjective\n",
      ",\tPunctuation\n",
      "잎새\tNoun\n",
      "에\tJosa\n",
      "이는\tVerb\n",
      "바람\tNoun\n",
      "에도\tJosa\n",
      "나\tNoun\n",
      "는\tJosa\n",
      "괴로워\tAdjective\n",
      "했다\tVerb\n",
      ".\tPunctuation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for word, pos in text_pos:\n",
    "    print(\"{}\\t{}\".format(word, pos))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 튜플의 자동 언패킹(unpacking) 기능 활용법\n",
    "\n",
    "리스트의 원소인 튜플을 하나씩 접근한 후, 개별 튜플의 원소들을 각각 출력하기 위해서는 다음과 같이 for문을 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1\n",
      "b 2\n",
      "c 3\n",
      "d 4\n",
      "e 5\n"
     ]
    }
   ],
   "source": [
    "A = [('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5)]\n",
    "\n",
    "for i in A:\n",
    "    print(i[0], i[1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1\n",
      "b 2\n",
      "c 3\n",
      "d 4\n",
      "e 5\n"
     ]
    }
   ],
   "source": [
    "for first, second in A:\n",
    "    print(first, second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번엔 `morphs() 메소드`와 `nouns() 메소드`를 적용해 보면, 결과에서 보듯 문장에서 분절된 형태소와 분절된 형태소들 가운데 명사만을 각각 리스트 내 원소로 반환해 준다. (텍스트 분석에서 가장 많이 사용되는 품사가 명사이기 때문에 명사만 따로 메소드 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['죽는', '날', '까지', '하늘', '을', '우러러', '한', '점', '부끄럼', '이', '없기를', ',', '잎새', '에', '이는', '바람', '에도', '나', '는', '괴로워', '했다', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\"\n",
    "\n",
    "text_words = twitter.morphs(text)\n",
    "\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['날', '하늘', '우러러', '점', '부끄럼', '잎새', '바람', '나']\n"
     ]
    }
   ],
   "source": [
    "text = \"죽는 날까지 하늘을 우러러 한 점 부끄럼이 없기를, 잎새에 이는 바람에도 나는 괴로워했다.\"\n",
    "\n",
    "text_nouns = twitter.nouns(text)\n",
    "\n",
    "print(text_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 텍스트를 셀 내에서 직접 입력한 후 형태소분석 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1:  ['이', '화', '여', '자', '대', '학', '교']\n",
      "a2:  ['이', '화', '여', '자', '대', '학', '교']\n"
     ]
    }
   ],
   "source": [
    "a= \"이 화 여 자 대 학 교\"\n",
    "a1= a.split(\" \")\n",
    "\n",
    "print(\"a1: \", a1)\n",
    "\n",
    "a2 =[]\n",
    "\n",
    "for word in a1:\n",
    "#    print(word)\n",
    "#     a2.append(word)\n",
    "    a2 += word\n",
    "    \n",
    "print(\"a2: \", a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(\"'\", 'SS'), ('눈', 'NNG'), ('이', 'JKS'), ('부시', 'VA'), ('게', 'ECD'), (\"'\", 'SS'), ('가', 'VV'), ('아', 'ECS'), ('가뿐', 'XR'), ('하', 'XSA'), ('게', 'ECD'), ('지상파', 'NNG'), ('월화', 'NNG'), ('극', 'NNG'), ('을', 'JKO'), ('따돌리', 'VV'), ('며', 'ECE'), ('6', 'NR'), ('%', 'SW'), ('를', 'JKO'), ('돌파', 'NNG'), ('하', 'XSV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [], [('27', 'NR'), ('일', 'NNM'), ('시청률', 'NNG'), ('조사', 'NNG'), ('회사', 'NNG'), ('니', 'VV'), ('ㄹ', 'ETD'), ('스', 'VV'), ('ㄴ', 'ETD'), ('코리아', 'NNG'), ('에', 'JKM'), ('따르', 'VV'), ('면', 'ECE'), ('26', 'NR'), ('일', 'NNM'), ('방송', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETD'), ('JTBC', 'OL'), ('월화', 'NNG'), ('극', 'NNG'), (\"'\", 'SS'), ('눈', 'NNG'), ('이', 'JKS'), ('부시', 'VA'), ('게', 'ECD'), (\"'\", 'SS'), ('늘', 'VV'), ('ㄴ', 'ETD'), ('6.567', 'NR'), ('%', 'SW'), ('(', 'SS'), ('전국', 'NNG'), ('유료', 'NNG'), ('가구', 'NNG'), ('기준', 'NNG'), (')', 'SS'), ('의', 'NNG'), ('시청률', 'NNG'), ('을', 'JKO'), ('기록', 'NNG'), ('?', 'SF')], [('5', 'NR'), ('회', 'NNM'), ('연속', 'NNG'), ('자체', 'NNG'), ('최고', 'NNG'), ('시청률', 'NNG'), ('을', 'JKO'), ('찍', 'VV'), ('으며', 'ECE'), ('멈추', 'VV'), ('ㄹ', 'ETD'), ('줄', 'NNB'), ('모르', 'VV'), ('는', 'ETD'), ('상승세', 'NNG'), ('를', 'JKO'), ('잇', 'VV'), ('어', 'ECD'), ('가', 'VV'), ('고', 'ECE'), ('있', 'VXV'), ('다', 'EFN'), ('!', 'SF')], [('동시', 'NNG'), ('에', 'JKM'), ('첫', 'MDT'), ('6', 'NR'), ('%', 'SW'), ('대', 'NNB'), ('돌파', 'NNG'), ('이', 'VCP'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [('동', 'NNG'), ('시간대', 'NNG'), ('방송', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETD'), ('지상파', 'NNG'), ('3', 'NR'), ('사', 'NR'), ('월', 'NNM'), ('화극', 'NNG'), ('SBS', 'OL'), (\"'\", 'SS'), ('해치', 'NNG'), (\"'\", 'SS'), ('KBS', 'OL'), ('2', 'NR'), ('TV', 'OL'), (\"'\", 'SS'), ('동네', 'NNG'), ('변호사', 'NNG'), ('조', 'NNG'), ('들', 'XSN'), ('호', 'NNB'), ('2', 'NR'), (':', 'SP'), ('죄', 'NNG'), ('와', 'JC'), ('벌', 'NNG'), (\"'\", 'SS'), ('MBC', 'OL'), (\"'\", 'SS'), ('아이템', 'NNG'), (\"'\", 'SS'), ('을', 'NNG'), ('따돌리', 'VV'), ('고', 'ECE'), ('우위', 'NNG'), ('를', 'JKO'), ('점하', 'VV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [('tvN', 'OL'), (\"'\", 'SS'), ('왕', 'NNG'), ('이', 'JKC'), ('되', 'VV'), ('ㄴ', 'ETD'), ('남자', 'NNG'), (\"'\", 'SS'), ('(', 'SS'), ('9.5', 'NR'), ('%', 'SW'), (')', 'SS'), ('를', 'JKO'), ('잇', 'VV'), ('는', 'ETD'), ('월화', 'NNG'), ('극', 'NNG'), ('전체', 'NNG'), ('2', 'NR'), ('위', 'NNM'), ('에', 'JKM'), ('이름', 'NNG'), ('을', 'JKO'), ('올리', 'VV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')], [(\"'\", 'SS'), ('왕', 'NNG'), ('이', 'JKC'), ('되', 'VV'), ('ㄴ', 'ETD'), ('남자', 'NNG'), (\"'\", 'SS'), ('의', 'NNG'), ('경우', 'NNG'), ('종영', 'NNG'), ('을', 'JKO'), ('앞두', 'VV'), ('고', 'ECE'), ('있', 'VXV'), ('기에', 'ECD'), (\"'\", 'SS'), ('눈', 'NNG'), ('이', 'JKS'), ('부시', 'VA'), ('게', 'ECD'), (\"'\", 'SS'), ('가', 'VV'), ('아', 'ECS'), ('어디', 'NP'), ('까지', 'JX'), ('상승', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('을지', 'ECS'), ('주목', 'NNG'), ('되', 'XSV'), ('ㄴ다', 'EFN'), ('!', 'SF')], [], [('이날', 'NNG'), ('방송', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('김', 'NNG'), ('혜', 'UN'), ('자', 'NNG'), ('(', 'SS'), ('김', 'NNG'), ('혜', 'UN'), ('자', 'NNG'), (')', 'SS'), ('가', 'NNG'), ('방송', 'NNG'), ('말미', 'NNG'), ('시간', 'NNG'), ('을', 'JKO'), ('되돌리', 'VV'), ('는', 'ETD'), ('시계', 'NNG'), ('를', 'JKO'), ('발견', 'NNG'), ('하', 'XSV'), ('는', 'ETD'), ('모습', 'NNG'), ('이', 'JKS'), ('그려지', 'VV'), ('었', 'EPT'), ('다', 'EFN'), ('?', 'SF')], [('전무', 'NNG'), ('송', 'NNG'), ('이', 'JKS'), ('이', 'MDT'), ('시계', 'NNG'), ('를', 'JKO'), ('차', 'VV'), ('고', 'ECE'), ('있었', 'VV'), ('고', 'ECE'), ('시계', 'NNG'), ('를', 'JKO'), ('보', 'VV'), ('ㄴ', 'ETD'), ('후', 'NNG'), ('눈빛', 'NNG'), ('이', 'JKS'), ('심하', 'VA'), ('게', 'ECD'), ('흔들리', 'VV'), ('ㄴ', 'ETD'), ('김혜자', 'UN'), ('의', 'JKG'), ('모습', 'NNG'), ('을', 'JKO'), ('통하', 'VV'), ('어', 'ECS'), ('다시금', 'MAG'), ('시간', 'NNG'), ('을', 'JKO'), ('되돌리', 'VV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('을지', 'ECS'), ('여부', 'NNG'), ('에', 'JKM'), ('관심', 'NNG'), ('이', 'JKS'), ('쏠리', 'VV'), ('었', 'EPT'), ('다', 'EFN'), ('.', 'SF')]]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "def split_sentences(text):\n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_pos(analyzer, text):\n",
    "\n",
    "    morph_anals = []\n",
    "    sentences = split_sentences(text)                       # 형태소분석 전에 문장 단위로 분리. 위에서 정의한 split_sentences 호출 \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        morph_anal = analyzer.pos(sentence)                 # 문장 단위로 형태소 분석하여 morph_anal의 출력 값 = [(word, pos)] \n",
    "        morph_anals.append(morph_anal)\n",
    "        \n",
    "    return morph_anals\n",
    "\n",
    "# main \n",
    "\n",
    "textdata = \"\"\"\n",
    "'눈이 부시게'가 가뿐하게 지상파 월화극을 따돌리며 6%를 돌파했다. \n",
    "27일 시청률 조사회사 닐슨 코리아에 따르면 26일 방송된 JTBC 월화극 '눈이 부시게'는 6.567%(전국 유료가구 기준)의 시청률을 기록? 5회 연속 자체 최고 시청률을 찍으며 멈출 줄 모르는 상승세를 이어가고 있다!\n",
    "동시에 첫 6%대 돌파였다. 동 시간대 방송된 지상파 3사 월화극 SBS '해치' KBS 2TV '동네변호사 조들호2:죄와 벌' MBC '아이템'을 따돌리고 우위를 점했다. tvN '왕이 된 남자'(9.5%)를 잇는 월화극 전체 2위에 이름을 올렸다. '왕이 된 남자'의 경우 종영을 앞두고 있기에 '눈이 부시게'가 어디까지 상승할 수 있을지 주목된다! \n",
    "이날 방송에는 김혜자(김혜자)가 방송 말미 시간을 되돌리는 시계를 발견하는 모습이 그려졌다? 전무송이 이 시계를 차고 있었고 시계를 본 후 눈빛이 심하게 흔들린 김혜자의 모습을 통해 다시금 시간을 되돌릴 수 있을지 여부에 관심이 쏠렸다. \n",
    "\"\"\"\n",
    "\n",
    "kkma = Kkma()\n",
    "textdata_pos = get_pos(kkma, textdata)\n",
    "print(textdata_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.0\n"
     ]
    }
   ],
   "source": [
    "import konlpy\n",
    "\n",
    "print(konlpy.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 외부 텍스트 파일을 불러와 형태소분석 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[(\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('가뿐하게', 'Adjective'), ('지상파', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('을', 'Josa'), ('따돌리며', 'Verb'), ('6%', 'Number'), ('를', 'Noun'), ('돌파', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')]], [[('27일', 'Number'), ('시청률', 'Noun'), ('조사', 'Noun'), ('회사', 'Noun'), ('닐슨', 'Noun'), ('코리아', 'Noun'), ('에', 'Josa'), ('따르면', 'Verb'), ('26일', 'Number'), ('방송', 'Noun'), ('된', 'Verb'), ('JTBC', 'Alpha'), ('월화', 'Noun'), ('극', 'Suffix'), (\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('는', 'Verb'), ('6.567%', 'Number'), ('(', 'Foreign'), ('전국', 'Noun'), ('유료', 'Noun'), ('가구', 'Noun'), ('기준', 'Noun'), (')', 'Punctuation'), ('의', 'Noun'), ('시청률', 'Noun'), ('을', 'Josa'), ('기록', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')]], [[('5회', 'Number'), ('연속', 'Noun'), ('자체', 'Noun'), ('최고', 'Noun'), ('시청률', 'Noun'), ('을', 'Josa'), ('찍으며', 'Verb'), ('멈출', 'Verb'), ('줄', 'Noun'), ('모르는', 'Verb'), ('상승세', 'Noun'), ('를', 'Josa'), ('이', 'Determiner'), ('어가', 'Noun'), ('고', 'Josa'), ('있다', 'Adjective'), ('.', 'Punctuation')], [('동시', 'Noun'), ('에', 'Josa'), ('첫', 'Noun'), ('6%', 'Number'), ('대', 'Verb'), ('돌파', 'Noun'), ('였다', 'Verb'), ('.', 'Punctuation')]], [[('동', 'Modifier'), ('시간대', 'Noun'), ('방송', 'Noun'), ('된', 'Verb'), ('지상파', 'Noun'), ('3', 'Number'), ('사', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('SBS', 'Alpha'), (\"'\", 'Punctuation'), ('해치', 'Noun'), (\"'\", 'Punctuation'), ('KBS', 'Alpha'), ('2', 'Number'), ('TV', 'Alpha'), (\"'\", 'Punctuation'), ('동네', 'Noun'), ('변호사', 'Noun'), ('조', 'Noun'), ('들', 'Suffix'), ('호', 'Noun'), ('2', 'Number'), (':', 'Punctuation'), ('죄', 'Noun'), ('와', 'Josa'), ('벌', 'Noun'), (\"'\", 'Punctuation'), ('MBC', 'Alpha'), (\"'\", 'Punctuation'), ('아이템', 'Noun'), (\"'\", 'Punctuation'), ('을', 'Josa'), ('따돌리고', 'Verb'), ('우위', 'Noun'), ('를', 'Josa'), ('점', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')]], [[('tvN', 'Alpha'), (\"'\", 'Punctuation'), ('왕', 'Noun'), ('이', 'Josa'), ('된', 'Verb'), ('남자', 'Noun'), (\"'(\", 'Punctuation'), ('9.5%', 'Number'), (')', 'Foreign'), ('를', 'Noun'), ('잇는', 'Verb'), ('월화', 'Noun'), ('극', 'Suffix'), ('전체', 'Noun'), ('2', 'Number'), ('위', 'Noun'), ('에', 'Josa'), ('이름', 'Noun'), ('을', 'Josa'), ('올렸다', 'Verb'), ('.', 'Punctuation')], [(\"'\", 'Punctuation'), ('왕', 'Noun'), ('이', 'Josa'), ('된', 'Verb'), ('남자', 'Noun'), (\"'\", 'Punctuation'), ('의', 'Noun'), ('경우', 'Noun'), ('종영', 'Noun'), ('을', 'Josa'), ('앞두고', 'Verb'), ('있기에', 'Adjective'), (\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('어디', 'Noun'), ('까지', 'Josa'), ('상승', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있을지', 'Adjective'), ('주목', 'Noun'), ('된다', 'Verb'), ('.', 'Punctuation')]], [[('이', 'Determiner'), ('날', 'Noun'), ('방송', 'Noun'), ('에는', 'Josa'), ('김혜자', 'Noun'), ('(', 'Punctuation'), ('김혜자', 'Noun'), (')', 'Punctuation'), ('가', 'Verb'), ('방송', 'Noun'), ('말미', 'Noun'), ('시간', 'Noun'), ('을', 'Josa'), ('되돌리는', 'Verb'), ('시계', 'Noun'), ('를', 'Josa'), ('발견', 'Noun'), ('하는', 'Verb'), ('모습', 'Noun'), ('이', 'Josa'), ('그려졌다', 'Verb'), ('.', 'Punctuation')], [('전무송', 'Noun'), ('이', 'Josa'), ('이', 'Noun'), ('시계', 'Noun'), ('를', 'Josa'), ('차고', 'Noun'), ('있었고', 'Adjective'), ('시계', 'Noun'), ('를', 'Josa'), ('본', 'Verb'), ('후', 'Noun'), ('눈빛', 'Noun'), ('이', 'Josa'), ('심하게', 'Adjective'), ('흔들린', 'Verb'), ('김혜자', 'Noun'), ('의', 'Josa'), ('모습', 'Noun'), ('을', 'Josa'), ('통해', 'Noun'), ('다시금', 'Noun'), ('시간', 'Noun'), ('을', 'Josa'), ('되돌릴', 'Verb'), ('수', 'Noun'), ('있을지', 'Adjective'), ('여부', 'Noun'), ('에', 'Josa'), ('관심', 'Noun'), ('이', 'Josa'), ('쏠렸다', 'Verb'), ('.', 'Punctuation')]]]\n"
     ]
    }
   ],
   "source": [
    "# from konlpy.tag import Twitter\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "def split_sentences(text):\n",
    "    text = text.strip().replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_pos(analyzer, text):\n",
    "\n",
    "    morph_anals = []\n",
    "    sentences = split_sentences(text)                       # 형태소분석 전에 문장 단위로 분리. 위 함수 split_sentences 호출 \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        morph_anal = analyzer.pos(sentence)            # 문장 단위로 형태소분석하여 word와 pos를 출력 \n",
    "        morph_anals.append(morph_anal)\n",
    "        \n",
    "    return morph_anals\n",
    "\n",
    "\n",
    "\n",
    "# main \n",
    "\n",
    "input_file_name = r\".\\Data\\textdata.txt\"\n",
    "\n",
    "# twitter = Twitter()\n",
    "twitter = Okt()\n",
    "\n",
    "     \n",
    "textdata_pos = []\n",
    "\n",
    "with open(input_file_name, \"r\", encoding=\"EUC-KR\") as input_file:\n",
    "    for line in input_file:\n",
    "        words_pos = get_pos(twitter, line)  # 앞서 정의한 사용자 함수 def split_sentences를 호출해 매개변수에 line을 입력\n",
    "        textdata_pos.append(words_pos)\n",
    "        \n",
    "print(textdata_pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "품사 태깅 결과: [(\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('가뿐하게', 'Adjective'), ('지상파', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('을', 'Josa'), ('따돌리며', 'Verb'), ('6%', 'Number'), ('를', 'Noun'), ('돌파', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation'), ('\\n', 'Foreign'), ('27일', 'Number'), ('시청률', 'Noun'), ('조사', 'Noun'), ('회사', 'Noun'), ('닐슨', 'Noun'), ('코리아', 'Noun'), ('에', 'Josa'), ('따르면', 'Verb'), ('26일', 'Number'), ('방송', 'Noun'), ('된', 'Verb'), ('JTBC', 'Alpha'), ('월화', 'Noun'), ('극', 'Suffix'), (\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('는', 'Verb'), ('6.567%', 'Number'), ('(', 'Foreign'), ('전국', 'Noun'), ('유료', 'Noun'), ('가구', 'Noun'), ('기준', 'Noun'), (')', 'Punctuation'), ('의', 'Noun'), ('시청률', 'Noun'), ('을', 'Josa'), ('기록', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation'), ('5회', 'Number'), ('연속', 'Noun'), ('자체', 'Noun'), ('최고', 'Noun'), ('시청률', 'Noun'), ('을', 'Josa'), ('찍으며', 'Verb'), ('멈출', 'Verb'), ('줄', 'Noun'), ('모르는', 'Verb'), ('상승세', 'Noun'), ('를', 'Josa'), ('이', 'Determiner'), ('어가', 'Noun'), ('고', 'Josa'), ('있다', 'Adjective'), ('.', 'Punctuation'), ('동시', 'Noun'), ('에', 'Josa'), ('첫', 'Noun'), ('6%', 'Number'), ('대', 'Verb'), ('돌파', 'Noun'), ('였다', 'Verb'), ('.', 'Punctuation'), ('동', 'Modifier'), ('시간대', 'Noun'), ('방송', 'Noun'), ('된', 'Verb'), ('지상파', 'Noun'), ('3', 'Number'), ('사', 'Noun'), ('월화', 'Noun'), ('극', 'Suffix'), ('SBS', 'Alpha'), (\"'\", 'Punctuation'), ('해치', 'Noun'), (\"'\", 'Punctuation'), ('KBS', 'Alpha'), ('2', 'Number'), ('TV', 'Alpha'), (\"'\", 'Punctuation'), ('동네', 'Noun'), ('변호사', 'Noun'), ('조', 'Noun'), ('들', 'Suffix'), ('호', 'Noun'), ('2', 'Number'), (':', 'Punctuation'), ('죄', 'Noun'), ('와', 'Josa'), ('벌', 'Noun'), (\"'\", 'Punctuation'), ('MBC', 'Alpha'), (\"'\", 'Punctuation'), ('아이템', 'Noun'), (\"'\", 'Punctuation'), ('을', 'Josa'), ('따돌리고', 'Verb'), ('우위', 'Noun'), ('를', 'Josa'), ('점', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation'), ('tvN', 'Alpha'), (\"'\", 'Punctuation'), ('왕', 'Noun'), ('이', 'Josa'), ('된', 'Verb'), ('남자', 'Noun'), (\"'(\", 'Punctuation'), ('9.5%', 'Number'), (')', 'Foreign'), ('를', 'Noun'), ('잇는', 'Verb'), ('월화', 'Noun'), ('극', 'Suffix'), ('전체', 'Noun'), ('2', 'Number'), ('위', 'Noun'), ('에', 'Josa'), ('이름', 'Noun'), ('을', 'Josa'), ('올렸다', 'Verb'), ('.', 'Punctuation'), (\"'\", 'Punctuation'), ('왕', 'Noun'), ('이', 'Josa'), ('된', 'Verb'), ('남자', 'Noun'), (\"'\", 'Punctuation'), ('의', 'Noun'), ('경우', 'Noun'), ('종영', 'Noun'), ('을', 'Josa'), ('앞두고', 'Verb'), ('있기에', 'Adjective'), (\"'\", 'Punctuation'), ('눈', 'Noun'), ('이', 'Josa'), ('부시', 'Noun'), ('게', 'Josa'), (\"'\", 'Punctuation'), ('가', 'Verb'), ('어디', 'Noun'), ('까지', 'Josa'), ('상승', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있을지', 'Adjective'), ('주목', 'Noun'), ('된다', 'Verb'), ('.', 'Punctuation'), ('이', 'Determiner'), ('날', 'Noun'), ('방송', 'Noun'), ('에는', 'Josa'), ('김혜자', 'Noun'), ('(', 'Punctuation'), ('김혜자', 'Noun'), (')', 'Punctuation'), ('가', 'Verb'), ('방송', 'Noun'), ('말미', 'Noun'), ('시간', 'Noun'), ('을', 'Josa'), ('되돌리는', 'Verb'), ('시계', 'Noun'), ('를', 'Josa'), ('발견', 'Noun'), ('하는', 'Verb'), ('모습', 'Noun'), ('이', 'Josa'), ('그려졌다', 'Verb'), ('.', 'Punctuation'), ('전무송', 'Noun'), ('이', 'Josa'), ('이', 'Noun'), ('시계', 'Noun'), ('를', 'Josa'), ('차고', 'Noun'), ('있었고', 'Adjective'), ('시계', 'Noun'), ('를', 'Josa'), ('본', 'Verb'), ('후', 'Noun'), ('눈빛', 'Noun'), ('이', 'Josa'), ('심하게', 'Adjective'), ('흔들린', 'Verb'), ('김혜자', 'Noun'), ('의', 'Josa'), ('모습', 'Noun'), ('을', 'Josa'), ('통해', 'Noun'), ('다시금', 'Noun'), ('시간', 'Noun'), ('을', 'Josa'), ('되돌릴', 'Verb'), ('수', 'Noun'), ('있을지', 'Adjective'), ('여부', 'Noun'), ('에', 'Josa'), ('관심', 'Noun'), ('이', 'Josa'), ('쏠렸다', 'Verb'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()\n",
    "\n",
    "input_file_name = r\".\\Data\\textdata.txt\"\n",
    "\n",
    "f = open(input_file_name, \"r\", encoding=\"EUC-KR\")\n",
    "\n",
    "print('품사 태깅 결과:', t.pos(f.read()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고: 영어 텍스트 분석 라이브러리 spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'an', 'unexpected', 'thing', '.']\n",
      "['It - PRON', \"'s - AUX\", 'an - DET', 'unexpected - ADJ', 'thing - NOUN', '. - PUNCT']\n",
      "Samsung 69 76 ORG\n",
      "South Korea 100 111 GPE\n"
     ]
    }
   ],
   "source": [
    "# advanced open source NLP library spacy(advanced NLP techniques, larger volume of text, NLTK와 CoreNLP는 교육 및 연구용)\n",
    "# anaconda prompt 관리자 권한으로 실행. 한글은 지원 안됨\n",
    "# conda install -c conda-forge spacy\n",
    "# python -m spacy download en # default English model (~50MB)   # nlp=spacy.load('en')\n",
    "# python -m spacy download en_core_web_md # larger English model (~1GB) 다운로드함\n",
    "# sm/md/lg (small, medium, large) model, The difference lies in accuracy and loading time.\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "txt = \"It's an unexpected thing.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "\n",
    "# doc = nlp(text)\n",
    "tokens_with_POS = [token.text + \" - \" + token.pos_ for token in doc]\n",
    "print(tokens_with_POS)\n",
    "\n",
    "# NER(Named Entity Recognition)\n",
    "text = \"\"\"Most of the outlay will be at home. No surprise there, either. While Samsung has expanded overseas, South Korea is still host to most of its factories and research engineers. \"\"\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"97e80d4d3ee94e89b2816177ccde7efe-0\" class=\"displacy\" width=\"650\" height=\"212.0\" direction=\"ltr\" style=\"max-width: none; height: 212.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"122.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"122.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">love</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"122.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">you,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"122.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">Juliet.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-97e80d4d3ee94e89b2816177ccde7efe-0-0\" stroke-width=\"2px\" d=\"M70,77.0 C70,2.0 200.0,2.0 200.0,77.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-97e80d4d3ee94e89b2816177ccde7efe-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,79.0 L62,67.0 78,67.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-97e80d4d3ee94e89b2816177ccde7efe-0-1\" stroke-width=\"2px\" d=\"M220,77.0 C220,2.0 350.0,2.0 350.0,77.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-97e80d4d3ee94e89b2816177ccde7efe-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M350.0,79.0 L358.0,67.0 342.0,67.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-97e80d4d3ee94e89b2816177ccde7efe-0-2\" stroke-width=\"2px\" d=\"M370,77.0 C370,2.0 500.0,2.0 500.0,77.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-97e80d4d3ee94e89b2816177ccde7efe-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M500.0,79.0 L508.0,67.0 492.0,67.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I love you, \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Juliet\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "text1 = 'I love you, Juliet.'\n",
    "\n",
    "nlp=spacy.load('en_core_web_md')   # NLTK에서는 dependency 그래프 그리기가 약간 더 복잡\n",
    "displacy.render(nlp(text1),style='dep', jupyter=True, options ={'distance':150})\n",
    "\n",
    "displacy.render(nlp(text1),style=\"ent\",jupyter=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
